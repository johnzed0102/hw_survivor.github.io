---
mode: agent
description: >
  Build an auto-updating arXiv feed page (GitHub Pages + nightly GitHub Actions),
  using Copilot CLI agent infrastructure (agents/skills/prompts) with a reproducible verify loop.
tools:
  - terminal
  - filesystem
  - git
---

You are a terminal-based Copilot CLI agent operating on my existing GitHub Pages repository.

PROJECT GOAL (HW1 Problem 3):
Add a new page to my website that displays the latest arXiv papers matching chosen keywords.
Each entry must show: title, authors, abstract, and a direct PDF link.
The list must auto-refresh every midnight via a GitHub Actions workflow.
A link to this page must appear on the homepage.

CURRENT CONTEXT:
- The repository is a GitHub Pages site with `index.html` at repo root.
- `/pacman/index.html` already exists and is linked from the homepage.
- We will add a new directory `/arxiv/` for the arXiv feed page.

NON-NEGOTIABLE REQUIREMENTS:
1) Create `/arxiv/` page accessible from the public GitHub Pages URL.
2) The page shows latest papers with: title, authors, abstract, PDF link.
3) Data auto-updates nightly at 00:00 UTC via GitHub Actions.
4) Homepage `index.html` contains a working link to `/arxiv/` (must work on GitHub Pages and local server).
5) The repo MUST include `.github` directory with: prompts/agents/skills + workflow used for this problem.

COURSE-CONFORMANT AGENT INFRASTRUCTURE (must follow):
- Use `.github/agent/` for agent definitions
- Use `.github/skills/<skill-name>/SKILL.md` for skills
- Use `.github/prompts/` for prompt runners
- Use `.github/workflows/` for GitHub Actions
- Use a “prove it works” loop: run commands, inspect outputs, fix, iterate

DECISIONS (already chosen; do NOT ask the user):
- arXiv query scope:
  - Categories: cs.AI, cs.CL, cs.LG
  - Keywords: "table" OR "structured data" OR "time series" OR "forecasting"
- Display count: 20 papers
- Fetch implementation: Python (standard library only)
- Data format: committed JSON file at `/arxiv/papers.json` generated by the workflow
- Frontend: vanilla HTML/CSS/JS (no frameworks), render JSON client-side

HIGH-LEVEL ARCHITECTURE OVERVIEW:
- Data layer: `scripts/fetch_arxiv.py` fetches arXiv Atom feed, parses fields, writes `/arxiv/papers.json`.
- Web layer: `/arxiv/index.html` loads `/arxiv/papers.json` and renders a clean feed with expandable abstracts.
- Automation: `.github/workflows/update-arxiv.yml` runs nightly, regenerates JSON, commits only if changed.
- Integration: homepage `index.html` link points to `./arxiv/index.html` (explicit file to avoid file:// directory listing).

FLOW OF CONTROL (mirror course case study):
1) Trigger: user runs a prompt or workflow runs on schedule.
2) Orchestration: an agent reads its agent file and executes steps in order.
3) Skill Consultation: agent consults SKILL.md for exact commands/implementation details.
4) Execution: agent uses terminal/filesystem/git tools to create/edit files and run validations.
5) Iteration: if any validation fails, fix and re-run until pass.

YOU MUST PRODUCE (as a plan + scaffolding actions):
A) Create/modify files with exact paths and responsibilities.
B) Create at least 3 agents and at least 4 skills (mirroring modular design).
C) Create at least 3 runnable prompts (single-shot tasks).
D) Create the GitHub Actions workflow.
E) Add explicit validations after each stage.
F) Ensure everything works under GitHub Pages base path `https://<user>.github.io/<repo>/`.

FILE PLAN (must implement exactly these paths):
1) Website (new):
   - `arxiv/index.html`            # page shell + container
   - `arxiv/arxiv.js`              # fetch + render logic
   - `arxiv/arxiv.css` (optional)  # styling
   - `arxiv/papers.json`           # generated + committed
2) Data fetching (new):
   - `scripts/fetch_arxiv.py`      # fetch Atom feed, parse, write JSON deterministically
3) GitHub Actions (new):
   - `.github/workflows/update-arxiv.yml`  # nightly update job + commit
4) Agent infrastructure (new):
   - `.github/agent/arxiv-pipeline.agent.md`
   - `.github/agent/code-quality.agent.md`
   - `.github/agent/publisher.agent.md`
   - `.github/skills/arxiv-api/SKILL.md`
   - `.github/skills/json-schema/SKILL.md`
   - `.github/skills/site-render/SKILL.md`
   - `.github/skills/github-actions/SKILL.md`
   - `.github/skills/git-commit/SKILL.md`
5) Prompt runners (new):
   - `.github/prompts/fetch-arxiv.prompt.md`        # run fetch locally + validate json
   - `.github/prompts/render-arxiv-page.prompt.md`  # verify page renders locally
   - `.github/prompts/wire-workflow.prompt.md`      # create workflow + local sanity checks
6) Integration (modify):
   - `index.html`  # add/ensure link: `./arxiv/index.html`

IMPLEMENTATION DETAILS (hard constraints):
- `scripts/fetch_arxiv.py` MUST:
  - Use arXiv API Atom feed endpoint (query, start=0, max_results=20, sortBy=submittedDate, sortOrder=descending).
  - Parse:
    - title (string, collapsed whitespace)
    - authors (list of strings)
    - abstract/summary (string, collapsed whitespace)
    - pdf_url (prefer link rel="related" with type "application/pdf"; fallback to id + ".pdf")
    - updated (ISO string if present)
    - arxiv_id (string)
  - Output JSON must be stable/deterministic (sorted keys, UTF-8, newline at EOF).
  - If fetch fails, exit non-zero with clear error message.

- `/arxiv/index.html` + JS MUST:
  - fetch `papers.json` via relative path (same folder)
  - render a list of cards
  - each card shows: title (click -> pdf), authors, abstract (collapsed/expandable), updated date
  - handle empty/error states (display message, not blank page)

- Workflow `.github/workflows/update-arxiv.yml` MUST:
  - run daily at 00:00 UTC using `schedule`
  - also allow manual trigger with `workflow_dispatch`
  - setup Python 3.x
  - run `python scripts/fetch_arxiv.py`
  - `git diff` check: only commit and push if `/arxiv/papers.json` changed
  - set git user.name/email to a bot identity
  - use `GITHUB_TOKEN` (no secrets needed)

VALIDATION PLAN (must be explicit commands):
After implementing:
1) Local data generation:
   - `python3 scripts/fetch_arxiv.py`
   - verify: `test -s arxiv/papers.json`
   - verify JSON keys: use a small Python one-liner to assert required fields exist for all entries
2) Local site render:
   - `python3 -m http.server 8000`
   - open `http://localhost:8000/arxiv/index.html`
   - confirm at least 1 paper renders with title/authors/abstract/pdf link
3) Workflow sanity:
   - ensure `.github/workflows/update-arxiv.yml` exists in repo root
   - confirm workflow contains schedule + workflow_dispatch + diff-gated commit
4) Homepage link:
   - confirm `index.html` contains `href="./arxiv/index.html"` and works under local server

ACCEPTANCE CRITERIA (pass/fail):
- Visiting `/arxiv/index.html` on GitHub Pages displays a list of papers (>=1).
- Each paper shows title, authors, abstract, and a working PDF link.
- `arxiv/papers.json` is updated by GitHub Actions nightly (or by manual dispatch) and committed to repo.
- Homepage has a working link to `./arxiv/index.html`.
- `.github/` contains prompt/agent/skill markdown files plus the workflow file.

RISKS & MITIGATIONS:
- API/network failure: fail fast with non-zero exit + readable error; workflow re-runs next day.
- Atom parsing edge cases: implement robust parsing + fallbacks for pdf links.
- Empty commits: guard with `git diff --quiet`.
- GitHub Pages base path: always use relative paths and explicit `index.html` in links.

NOW EXECUTE THIS PLAN IN THE FOLLOWING ORDER:
1) Create the agent/skills/prompts scaffolding files (minimal but complete).
2) Implement `scripts/fetch_arxiv.py` and validate locally.
3) Implement `/arxiv/` page and validate locally.
4) Implement workflow and validate YAML + diff-gated commit logic.
5) Update homepage link and validate.
6) Summarize what was created and the exact commands to run for verification.
